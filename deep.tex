\newpage
\clearpage

\section{O BÁSICO DE REDES NEURAIS ARTIFICIAIS}
\label{deep}
Inspiradas no modelo de sistema nervoso de seres inteligentes, as redes neurais são compostas por um conjunto interconectado de neurônios de modo que, assim como o sistema humano, um neurônio, quando conectado com outros neurônios, pode comunicar impulsos nervosos por meio de sinapses. A representação do modelo humano pode ser observado na Figura \ref{deep:fig:1}.

\begin{figure}[H]
    \centering
    \caption{Representação do neurônio biológico.}
    \includegraphics[width=1\linewidth]{recursos/imagens/deep/neuronio.png}
    \label{deep:fig:1}

    Fonte: traduzido e adaptado de \cite{Stevens1979}.
\end{figure}

Dentre as representações matemáticas, destaca-se o modelo pioneiro desenvolvido por \cite{mcculloch1943logical}, que baseado no neurônio biológico busca definir uma saída binária, a partir de entradas que são recebidas por meio dos dendritos, sendo assim, sua representação matemática se dá pela equação:

\begin{equation}
    \label{deep:eq:1}
    J = \sum_{i = 1}^{n} x_i.
\end{equation}

Para a Equação \ref{deep:eq:1} e para a Figura \ref{deep:fig:2}, é importante dizer que $J$ representa a saída do neurônio, enquanto $x_i$ é a entrada a partir de $i$. Logo, é possível observar que por meio dos dendritos ocorre a entrada dos dados que é somada (ponderadamente) no núcleo $\sum_{i = 1}^{n} x_i$. Por fim, vale citar que $(x_1, ..., x_n)$ é assimilado aos dendritos e, além disso, é comum ver a representação do modelo supracitado através da (Figura \ref{deep:fig:2}).

\begin{figure}[H]
    \centering
    \caption{Representação gráfica de neurônio \cite{mcculloch1943logical}.}
    \includegraphics[width=1\linewidth]{recursos/imagens/deep/neuronio_mc.png}
    \label{deep:fig:2}

    Fonte: do próprio autor. Inspirado em \cite{mcculloch1943logical}.
\end{figure}

A partir do modelo de neurônio desenvolvido por \cite{mcculloch1943logical},  outros trabalhos foram desenvolvidos, no qual vale ressaltar o trabalho de \cite{Rosenblatt1958}, que propôs um modelo de neurônio e o nomeou de Perceptron, o qual tem como princípio o uso de uma saída binária. Logo, destaca-se que a sua função de ativação é dada de forma condicional e pode ser representada pela Equação \ref{deep:eq:2} \citep{Rosenblatt1958}:

\begin{equation}
    \label{deep:eq:2}
    A(Y') = \left\{\begin{matrix}
     1,& se \sum_{i = 1}^{N} w_i x_i + b \geq 0 \\ 
     0,& caso \;  contrario.
    \end{matrix}\right.
\end{equation}

Dessa forma, entende-se que $b$ representa o viés que determina um limiar para a ativação do neurônio. O vetor $\textbf{x}$ corresponde à entrada da rede, enquanto $\textbf{W}$ é a matriz de pesos que deve ser inicializada aleatoriamente e é ajustada no decorrer do treinamento.

Dentre as vantagens para o modelo Perceptron, pode-se salientar a sua aptidão para resolução de alguns problemas lógicos, basicamente problemas linearmente separáveis, todavia, em meio às suas limitações, reforça-se sua imperícia em relação a problemas não linearmente separáveis como os do exemplo na Figura \ref{deep:fig:3.2}.

\begin{figure}[H]
   \caption[Problemas de separabilidade.]{Representação de problemas linearmente e não linearmente separáveis.}
   \centering
   \label{deep:fig:3}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=1.5in]{recursos/imagens/deep/l_separavel.png}
        \caption{Problema linearmente separável.}
        \label{deep:fig:3.1}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=1.5in]{recursos/imagens/deep/nl_separavel.png}
        \caption{Problema não linearmente separável.}
        \label{deep:fig:3.2}
    \end{subfigure}%

    Fonte: do próprio autor.
\end{figure}


Para suprir essa necessidade de trabalhar com problemas não lineares, \cite{Werbos:74} realizou uma proposta de utilizar várias camadas de neurônios Perceptron juntos, de modo que obrigatoriamente possuísse camadas de entrada, ocultas e de saída. Assim, esse modelo - nomeado de \textit{Multi Layer Perceptron} (MLP) \citep{Werbos:74} e representado na Figura \ref{deep:fig:4} - ensejou o trabalho com funções de ativação não lineares e o desenvolvimento de técnicas de aprendizado, como a \textit{backpropagation}, desenvolvida por \cite{rumelhart1986learning}, a qual será detalhada na Seção \ref{deep:backprop}.

\begin{figure}[H]
    \centering
    \caption{Representação do modelo MLP.}
    \includegraphics[width=1\linewidth]{recursos/imagens/deep/mlp.png}
    \label{deep:fig:4}

    Fonte: do próprio autor.
\end{figure}

Entretanto, com o uso dos modelos citados, na tentativa de resolver problemas de nível de complexidade maior e do desenvolvimento de trabalhos que contribuíssem para o estado-da-arte, percebeu-se que ao adicionar mais camadas era possível trabalhar com uma base de dados maior e mais complexa, além de obter melhores resultados e uma etapa maior de treinamento, o que deu origem às intituladas redes neurais de aprendizado profundo, ou em inglês, \textit{deep learning} \citep{Goodfellow2016}. No que lhe concerne, essas possuem o princípio de aprender os parâmetros e representações dos dados de acordo com as suas interações \citep{ponti2018funciona}. 
Assim, permitindo que o algoritmo combine e simplifique os parâmetros antes de propagar as representações para as demais camadas \citep{Goodfellow2016}.

Além disso, vale citar que um diferencial dos modelos de \textit{deep learning} se materializa no fato de que seus neurônios se tornam especialistas em determinados objetivos e se tornam diferentes um dos outros, como o que ocorre com as CNNs, que têm neurônios responsáveis por extrair características, enquanto os neurônios da camada final fazem classificação (ou regressão), simulando o tráfego da informação no córtex visual humano.

Destarte, o uso e desenvolvimento de \textit{deep learning} tem se popularizado e sido considerado a resposta para problemas de diversas áreas, como segmentações em vários contextos, desenvolvimentos de carros autônomos, segurança, otimização de modelos de busca e afins \citep{Ghosh2019}, sendo que essa popularização é propícia devido: 1) ao desenvolvimento acelerado de \textit{hardwares} que acrescentam velocidade no treinamento dessas redes e 2) ao grande aumento de dados variados (com adventos como o \textit{big data}) \citep{Szegedy2015, ponti2018funciona}.

Por conseguinte, nas demais seções deste capítulo serão apresentadas algumas variações de funções de ativação (Seção \ref{deep:activation}), sobre o processo de treinamento (Seção \ref{deep:train}) e de testes (Seção \ref{deep:test}).


\subsection{Funções de Ativação}
\label{deep:activation}

Quanto às funções de ativação, destaca-se que na seção anterior foi apresentada uma função de ativação binária, todavia, existem outras funções de ativação que foram estudadas e são utilizadas para uma melhor adequação em contextos específicos.

A função de ativação, como citado brevemente, é calculada pelo neurônio e disseminada para os próximos neurônios.  Para essa Seção, o valor calculado $\sum_{i = 1}^{N} w_ix_i +b$ (entrada da função de ativação) será representado por $Y'$, de modo que poderemos ver sua atuação, a seguir, nas funções de ativação Limiar, Linear, Sigmóide, ReLU e Sofmax.


\subsubsection{Função Limiar}
\label{deep:limiar}
A função limiar define um limite (\textit{threshold}) $\gamma$, em que valores abaixo de $\gamma$ indicam uma saída sem excitação, para valores acima de $\gamma$, há a excitação para o neurônio, utilizando assim, de medidas binárias, que normalmente são representadas por 0 e 1, respectivamente.

Embora a função limiar seja eficaz ao lidar com problemas binários, sua aplicação torna-se desafiadora em cenários multiclasse. Isso se deve ao fato de que ela não assegura a ativação exclusiva de um único neurônio na camada de saída, dificultando a representação adequada de classes distintas.

Em relação à função de ativação ($f(Y')$) desenvolvida e utilizada no Perceptron, pode-se dizer que o valor de $\gamma = 0$. A função limiar é definida matematicamente por \cite{mcculloch1943logical}, segundo a Equação \ref{deep:eq:3}:

\begin{equation}
    \label{deep:eq:3}
    f(Y') = \begin{cases}
     1,& se \; Y' \geq \gamma \\ 
     0,& \text{caso contrário}
    \end{cases}.
\end{equation}


\subsubsection{Função Linear}
\label{deep:linear}
Para suprir a problemática de trabalhar com contextos de multiclasse, saídas contínuas podem ajudar na interpretação e mitigar o problema da função de limiar. Nesse contexto, é apropriado o uso da função linear. É importante notar que os resultados produzidos por funções lineares são também lineares, tornando-as inadequadas para problemas não lineares. A função linear é expressa pela Equação \ref{deep:eq:5}, onde $\alpha$ representa um parâmetro ajustável, conforme definido por \cite{Rosenblatt1958}:

\begin{equation}
\label{deep:eq:5}
f(Y') = \alpha Y'.
\end{equation}

O \textit{Adaptive Linear Neuron} (ADALINE) \citep{Widrow199030YO} e a regra de aprendizado de Widrow-Hoff são marcos históricos na evolução dos algoritmos de aprendizado de máquina. O ADALINE, representa uma evolução do Perceptron, sendo que esse função linear para computar sua saída a partir das entradas. Diferentemente do Perceptron, que atualiza seus pesos baseado em erros de classificação binários, o ADALINE ajusta os pesos através de uma função de erro contínuo, usando a diferença entre a saída esperada e a saída calculada pela função linear, o que representa uma abordagem mais granular para a correção dos pesos.

A regra de aprendizado de Widrow-Hoff, foi desenvolvida especificamente para o ADALINE. Ela se concentra em minimizar a soma dos quadrados dos erros entre as saídas desejadas e as saídas produzidas pela rede, um método que demonstra ser eficaz para ajustar os parâmetros ($\alpha$) do modelo de forma a reduzir esse erro. Este princípio é aplicado em algoritmos de aprendizado profundo, enfatizando a relevância histórica e prática do ADALINE e da regra de Widrow-Hoff na fundação das redes neurais modernas e no aprendizado de máquina como um todo.

Embora o ADALINE seja restrito a problemas lineares, seu desenvolvimento foi um passo cruciais para os modelos subsequentes, que generalizam a abordagem para contextos não lineares, aprimorando as capacidades de modelagem e de aprendizado das redes.


\subsubsection{Função Sigmoide}
\label{deep:sigmoide}
As funções sigmoide têm uma ampla aplicação além de problemas lineares, caracterizando-se como uma função de ativação não linear. Elas permitem a modelagem de probabilidade em relação à saída do neurônio.

Esta função de ativação também é conhecida como função logística. Sua expressão matemática é definida por \citep{glorot2011deep}:

\begin{equation}
    \label{deep:eq:6}
    f(Y') = \frac{1}{1 + e^{Y'}}.
\end{equation}

A desvantagem dessa função encontra-se em situações de variação do eixo x, em que há uma dissipação do seu gradiente, implicando em um aprendizado mais lento nos neurônios das primeiras camadas em relação aos das últimas, tendo em vista os valores muito pequenos no início \citep{Goodfellow2016}.

\subsubsection{Função ReLU}
\label{deep:relu}
A função desenvolvida por \cite{Hahnioser2000} e popularmente conhecida como ReLU (\textit{Rectified Linear Unit}) se faz útil nas camadas ocultas da rede neural \citep{Goodfellow2016}, sendo que sua utilidade é destacada por não permitir resultados negativos \citep{Dahl2013} e ter uma rápida convergência, principalmente quando comparada à função sigmoide, além de não passar por problemas de perda do gradiente. Sua função é definida pela Equação \ref{deep:eq:7} \citep{Hahnioser2000}:

\begin{equation}
    \label{deep:eq:7}
    f(Y') = \max(0,Y').
\end{equation}

\subsubsection{Função Softmax}
\label{deep:soft}
Por fim, quando se trata da função Softmax, dois processos são essenciais para a convergência do modelo em questão, sendo ele: 1) realizar a exponenciação dos valores de cada neurônio e 2) realizar a normalização de cada valor pela soma de todos os valores exponenciado no passo anterior, assim, definido uma saída no domínio $[0,1]$, fazendo com que a soma total seja sempre 1 \citep{kotu2018data} e que a saída da função seja a distribuição de probabilidade da classe. Esse comportamento garante que a função Softmax seja recomendada para problemas de multiclasses, além de determinar as probabilidades de cada uma das classes.

Em um problema de k-classes, a função Softmax pode ser expressa pela Equação \ref{deep:eq:8} \citep{kotu2018data}:

\begin{equation}
    \label{deep:eq:8}
    f(Y = \boldsymbol{k}) = \frac{e^{z_k}}{\sum_{i=1}^{\boldsymbol{k}} e^{Y'}},
\end{equation}
tendo $z_k$ como cada entrada do vetor $\boldsymbol{k}$.


\subsection{Treinamento}
\label{deep:train}

Definida como uma fase crucial por \cite{ponti2018funciona}, a fase de treinamento está relacionada com a minimização da função de custo (que será abordada na Seção \ref{deep:cust}), de modo que a rede ajuste os seus parâmetros iterativamente. Essas interações são explicadas por cada laço de repetição que ocorre com cada exemplo, sendo esperado que o erro propagado na rede seja minimizado. Assim, quando as interações ocorrem por todo o conjunto de dados de treinamento, é dito que aconteceu a ocorrência de uma época.

Normalmente, os passos realizados em uma época são os seguintes: 1) inicialização; 2) escolha de uma amostra (normalmente de modo aleatório); 3) cálculo de saída através de função de ativação; 4) cálculo do erro através de função de custo e \textit{backpropagation}; 5) modificação dos pesos de cada camada da rede e; 6) uma nova interação, até que tenha se passado por todas as instâncias do conjunto de treinamento.

Além da seleção aleatória, vale citar que normalmente os pesos são iniciados de modo aleatório nas camadas, assim como os hiperparâmetros, que são definidos pelo usuário na etapa de inicialização, por exemplo, o valor da taxa de aprendizado.

Nesta seção, será brevemente discorrido sobre as funções de custo (Seção \ref{deep:cust}), que indicam o erro da rede em relação ao conjunto de treinamento, sobre as funções de otimização (Seção \ref{deep:optimization}), que auxiliam na convergência do modelo e, por fim, sobre o algoritmo \textit{backpropagation} (Seção \ref{deep:backprop}), que propaga o erro para as camadas internas da rede.


\subsubsection{Função de Custo}
\label{deep:cust}

As funções de custo (do inglês, \textit{loss functions}) são responsáveis por realizar a avaliação do modelo durante a fase de treinamento, de certa maneira, avaliando quão perto o aprendizado do modelo está da resposta correta.

Em meio às avaliações utilizadas como métrica para o cálculo da função de custo em relação a problemas como o de regressão, destaca-se o erro quadrático médio (ou \textit{Mean Squared Error}, MSE)\citep{Wang2004}, em que para o âmbito de visão computacional e trabalho com imagens, comumente ocorre da saída da rede neural ser uma imagem, a partir de onde calcula-se o MSE dos \textit{pixels} existentes entre a imagem predita $\boldsymbol{y'}$ e a imagem original $\boldsymbol{y}$. Assim, supondo que M e N sejam as dimensões da imagem, o MSE é calculado pela equação:

\begin{equation}
    \label{deep:eq:9}
    MSE(\boldsymbol{y}, \boldsymbol{y'}) = \frac{1}{MN} \sum_{i=1}^{M} \sum_{j=1}^{N} (\boldsymbol{y}_{i,j} - \boldsymbol{y'}_{i,j})^2.
\end{equation}

Ainda quanto às métricas para a realização de avaliação em relação a imagens, existem várias delas descritas na literatura, todavia,  cada uma delas possui seu uso especificado para o problema que deseja resolver, por exemplo o caso das métricas para segmentações semânticas (Seção \ref{semantic:metrics}), que também poderia ser empregadas como função de custo para o treinamento.


\subsubsection{Função de Otimização}
\label{deep:optimization}

O grande objetivo das funções de otimização no contexto de aprendizado profundo, de certa forma, podem ser resumidos pela Equação \ref{deep:eq:10} e visualizada na Figura \ref{deep:fig:otimizacao}, em que almeja-se uma minimização global da função de custo $f(\boldsymbol{x})$ através de pesos $\boldsymbol{\theta}$ e de entradas $\boldsymbol{x}$:

\begin{equation}
    \label{deep:eq:10}
    \boldsymbol{\theta}* = argmin(f(\boldsymbol{x};\boldsymbol{\theta})).
\end{equation}

\begin{figure}[H]
    \centering
    \caption[Otimização da função de custo.]{Representação da otimização da função de custo em um espaço de características.}
    \includegraphics[width=1\linewidth]{recursos/imagens/deep/funcao_otimizacao.png}
    \label{deep:fig:otimizacao}

    Fonte: do próprio autor. Inspirado em \cite{MontesinosLopez2022ArtificialOutcomes} e \cite{Geron2017Hands-onSystems}.
\end{figure}


A Figura \ref{deep:fig:otimizacao} proporciona uma visualização de alguns desafios para as funções de otimização, sendo 1) mínimos locais e 2) platô (\textit{plateau}, em inglês). Um mínimo local representa um ponto que é o menor em comparação com outros pontos em sua vizinhança próxima. Entretanto, é importante notar que um mínimo local nem sempre corresponde ao menor valor em todo o espaço de características. Dessa forma, a função de custo pode apresentar vários mínimos locais, o que frequentemente dificulta a busca pelo mínimo global desejado na otimização \citep{Goodfellow2016}. Já os pontos de platô indicam áreas onde o valor da função de custo permanece estagnado. Nestes locais, os gradientes são muito semelhantes, o que torna desafiadora a otimização do modelo \citep{Goodfellow2016}.

Em meio às diversas funções disponíveis, considera-se que o método mais utilizado é o de gradiente descendente estocástico, ou em inglês, \textit{stochastic gradient descendent} (SGD), desenvolvido inicialmente por \cite{cauchy1847methode}, assim como suas variações \citep{Goodfellow2016}, sendo que o gradiente descendente estocástico é considerado como uma adaptação do gradiente descendente e que causa a aceleração do treinamento na maioria das vezes por selecionar aleatoriamente apenas uma parcela do conjunto de treinamento. Todavia, segundo \cite{Goodfellow2016}, há casos em que o SGD não tem bom desempenho para convergir e por conta disso são estudadas suas variações.

De qualquer modo é interessante ter consciência que até então não há um consenso na escolha do melhor método de otimização para problemas em gerais, segundo aborda \cite{Goodfellow2016}.


\paragraph{Gradiente Descendente}
\label{deep:optimization:graddesc}
O método do gradiente descendente representa uma técnica de otimização com a capacidade de encontrar soluções ideais em uma variedade de contextos, operando de maneira iterativa \citep{Geron2017Hands-onSystems}. Seu processo inicia-se com o cálculo dos gradientes locais resultados da função de custo $f(\boldsymbol{\theta})$, bem como com a atualização do valor dos pesos $\boldsymbol{\theta}$ com base na magnitude do gradiente e na direção indicada pelo cálculo das derivadas. A derivada da função $f'(\boldsymbol{\theta})$, determina a inclinação da função $f(\boldsymbol{\theta})$ no ponto $\boldsymbol{\theta}$, permitindo a análise de como a variação nos valores de entrada $\boldsymbol{x}$ da função se reflete em mudanças correspondentes nos valores de saída \citep{Goodfellow2016}, sendo possível minimizar a função movendo os valores de $\boldsymbol{\theta}$ em pequenos incrementos no sentido oposto ao da derivada.

Em situações envolvendo funções com múltiplas variáveis de entrada, as derivadas parciais $\frac{\partial}{\partial_{\theta_{i}}}f(\boldsymbol{\theta})$ são levadas em consideração para mensurar o quanto a função $f$ varia quando a entrada $\theta_i$ é alterada no ponto, $\boldsymbol{\theta}$. Assim, o gradiente passa a ser representado por um vetor que encapsula os valores de todas as derivadas parciais \citep{Goodfellow2016}, sendo expresso por: $\nabla_{\boldsymbol{\theta}} f(\boldsymbol{\theta})$. Por fim, a atualização dos parâmetros ocorre conforme a Equação \ref{deep:eq:graddesc} descrita por \cite{Goodfellow2016}:

\begin{equation}
    \label{deep:eq:graddesc}
    \boldsymbol{\theta}' = \boldsymbol{\theta} - \epsilon \nabla_{\boldsymbol{{\theta}}} f(\boldsymbol{\theta}).
\end{equation}

Dessa forma, o valor de entrada $\boldsymbol{\theta}$ é ajustado por meio da multiplicação das derivadas parciais pela taxa de aprendizado $\epsilon$, conforme mostrado na Figura \ref{deep:fig:otimizacao}, onde os círculos vermelhos indicam a evolução da taxa de aprendizado.

\paragraph{Gradiente Descendente Estocástico}
\label{deep:optimization:sgd}
O SGD é amplamente adotado devido à sua eficiência em modelos de aprendizado profundo e conjuntos de dados em larga escala \citep{Goodfellow2016}. No entanto, é importante mencionar suas limitações. O SGD não assegura a convergência e é sensível aos valores iniciais do modelo. No entanto, essas desvantagens não excluem sua utilidade, especialmente quando combinado com técnicas como normalização e o uso de \textit{momentum} \citep{Goodfellow2016}.

A fórmula do SGD emprega a derivada da função $f(\boldsymbol{x}; \boldsymbol{\theta})$ em relação aos parâmetros $\boldsymbol{\theta}$ para otimizar a saída $\boldsymbol{y'} = f(\boldsymbol{x})$. Esta atualização dos parâmetros, impulsionada pela derivada, visa melhorar a saída prevista $\boldsymbol{y'}$. A representação matemática do SGD é demonstrada na Equação \ref{deep:eq:sgd}, conforme proposto por \cite{Goodfellow2016}:

\begin{equation}
\label{deep:eq:sgd}
g = \nabla_{\boldsymbol{\theta}} \left(\frac{1}{m} \sum\limits_{i=1}^{m}L(f(\boldsymbol{x}^{(i)}; \boldsymbol{\theta}), \mathbf{y}^{(i)})\right),
\end{equation}
onde $L(x,y)$ é a função de custo, $\boldsymbol{x^{(i)}}$ representa a entrada, $\boldsymbol{y^{(i)}}$ indica a saída esperada, $m$ é o número de exemplos no conjunto de dados, e $f(\boldsymbol{x}^{(i)}; \boldsymbol{\theta})$ é a saída da rede para o enésimo exemplo $\boldsymbol{x}^{(i)}$ com o conjunto de pesos $\boldsymbol{\theta}$.

\paragraph{Parada antecipada}
\label{deep:optimization:early_stoping}

Ao configurar os hiperparâmetros de um modelo, é crucial considerar a quantidade de épocas pelas quais o conjunto de dados deve passar durante o treinamento. Essa escolha tem um impacto significativo nos fenômenos discutidos na Seção \ref{deep:overunder}. Optar por um número insuficiente de épocas frequentemente resulta em \textit{underfitting}, enquanto um número excessivo pode levar ao \textit{overfitting}. Surge, então, a pergunta fundamental: \quotes{Qual é o número ideal de épocas para cada problema?}

Para abordar essa questão, diversos estudos têm sido realizados com o intuito de encontrar critérios de parada no treinamento que minimizem ou previnam esses problemas \citep{Prechelt1998AutomaticCriteria}. Além disso, pesquisas recentes tentam antecipar situações de \textit{overfitting} e \textit{underfitting}, interrompendo o treinamento antes que afete o desempenho do modelo \citep{VilaresFerro2023EarlyNetworks}. Existem múltiplos métodos disponíveis para essa tarefa, sendo a parada antecipada (do inglês, \textit{early stopping}) um dos mais populares, podendo ser considerado uma forma de regularização, tal como o decaimento de pesos \citep{VilaresFerro2023EarlyNetworks}.

A aplicação da Parada Antecipada implica calcular a precisão da classificação nos dados de validação ao final de cada época \citep{Prechelt1998AutomaticCriteria}. Quando a precisão para de melhorar, encerramos o treinamento. Isso simplifica a definição do número de épocas, pois não é necessário explicitamente determinar como ele depende de outros hiperparâmetros - essa adaptação é feita automaticamente. Ademais, a Parada Antecipada atua como um mecanismo automático de prevenção ao \textit{overfitting}. Embora, em estágios iniciais de experimentação, possa ser útil desativar temporariamente a Parada Antecipada para identificar sinais de \textit{overfitting} e, assim, ajustar a abordagem de regularização.

A implementação efetiva da parada antecipada exige uma definição mais precisa do que significa \quotes{parar de melhorar} em termos de precisão da classificação. Como observado anteriormente, a precisão pode oscilar mesmo durante uma tendência geral de melhora. Parar imediatamente quando há uma diminuição na precisão pode interromper o processo quando melhorias adicionais ainda são possíveis \citep{Prechelt1998EarlyWhen}. Uma regra mais sensata para a parada antecipada $n(i)$ no treinamento é se a melhor precisão de classificação não melhorar por um determinado número de épocas consecutivas $j$ a partir da época de melhor acurácia $n_{Acc}$, como representado pela Equação \ref{deep:eq:early_stopping} e também pela Figura \ref{deep:fig:earlystopping}:

\begin{equation}
    \label{deep:eq:early_stopping}
    n(i) = n_{Acc} + j.
    % \text{Parada Antecipada} =\begin{cases}
    %     1,& \text{se $Acc$ não melhore por $n$ épocas consecutivas} \\
    %     0,& \text{caso contrário}
    % \end{cases}
\end{equation}


\begin{figure}[H]
    \centering
    \caption[Parada antecipada.]{Representação do momento ideal para parada antecipada.}
    \includegraphics[width=0.6\linewidth]{recursos/imagens/deep/early_stopping.png}
    \label{deep:fig:earlystopping}

    Fonte: do próprio autor.
\end{figure}


\paragraph{Taxa de Aprendizado Dinâmica}
\label{deep:optimization:dynamic_lr}

A taxa de aprendizado (\textit{learning rate}, em inglês), como demonstrado anteriormente, é um dos hiperparâmetros fundamentais utilizados na etapa inicial do treinamento de modelos. Sua representação como $\epsilon$ pode ser observada na Equação \ref{deep:eq:graddesc} e desempenha um papel crucial na determinação do tamanho dos passos das iterações para a otimização de gradientes \citep{Smith2017CyclicalNetworks}.

Definir a taxa de aprendizado, assim como abordado na situação de parada antecipada mencionada na Seção \ref{deep:optimization:early_stoping}, muitas vezes torna-se um desafio sem que tenha um veredito definido como estado da arte \citep{Goodfellow2016}. Isso se deve ao fato de que quanto maior o valor definido, mais acentuada é a atualização dos parâmetros, possibilitando uma exploração mais rápida do espaço de características, como ilustrado na Figura \ref{deep:fig:dynamic_lr.1}. No entanto, essa abordagem frequentemente resulta na perda do valor ótimo \citep{Tang2021AnPump}. Por outro lado, ao diminuir o valor da taxa de aprendizado, a atualização dos parâmetros torna-se mais gradual, o que ajuda a evitar a perda de mínimos globais, como apresentado na Figura \ref{deep:fig:dynamic_lr.2}. Contudo, é importante salientar que essa estratégia consome consideravelmente mais recursos computacionais e, muitas vezes, não atinge o valor mínimo global devido ao grande número de iterações necessárias para a convergência \citep{Tang2021AnPump}.

\begin{figure}[H]
    \centering
    \caption[Atualização da taxa de aprendizado.]{Representação de atualização da taxa de aprendizado.}
    \label{deep:fig:dynamic_lr}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{recursos/imagens/deep/lr_high.png}
        \caption{Taxa de aprendizado muito alta.}
        \label{deep:fig:dynamic_lr.1}
    \end{subfigure}%
    ~ 
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[width=1\linewidth]{recursos/imagens/deep/lr_low.png}
        \caption{Taxa de aprendizado muito baixa.}
        \label{deep:fig:dynamic_lr.2}
    \end{subfigure}

    Fonte: do próprio autor.
\end{figure}


Essas representações visuais na Figura \ref{deep:fig:dynamic_lr} evidenciam os impactos da variação da taxa de aprendizado na otimização do modelo. A escolha adequada desse hiperparâmetro é crucial para encontrar um equilíbrio entre a velocidade de convergência e a busca pelo valor ótimo da função de custo.

O aumento no número de estudos, como os de \cite{Tang2021AnPump, Senior2013AnRecognition} e \cite{Smith2017CyclicalNetworks}, que visam aprimorar técnicas para encontrar valores ótimos de taxas de aprendizado tem se tornado uma tendência crescente. Esses estudos se concentram na busca por equilibrar os parâmetros ou otimizar a definição desses valores. Muitos desses estudos adotam uma abordagem baseada na programação de taxas de aprendizado dinâmicas, através do uso de um agendador de taxa de aprendizado (\textit{learning rate scheduler}) - um método que define critérios para aumentar ou diminuir o valor da taxa de aprendizado $\epsilon$. Normalmente, esse valor é programado para diminuir linearmente, resultando em atualizações de pesos maiores no início da otimização e menores à medida que a otimização avança e o ponto mínimo se aproxima. A redução gradual da taxa de aprendizado pode ser descrita pela Equação \ref{deep:eq:dynamic_lr}, proposta por \cite{Goodfellow2016}:

\begin{equation}
\label{deep:eq:dynamic_lr}
\epsilon_i = (1 - \alpha) \epsilon_0 + \alpha\epsilon_\tau,
\end{equation}
sendo que nesta equação, $\tau$ representa o número de iterações em que $\epsilon$ será reduzido para uma constante, $i$ é a iteração atual, $\epsilon_0$ é a taxa de aprendizado inicial e $\alpha$ é determinado por $\frac{i}{\tau}$. Além disso, é importante destacar que o valor de $\tau$ é definido como o número de iterações necessárias para completar as épocas do conjunto de treinamento, enquanto $\epsilon_\tau$ é aproximadamente 1\% do valor de $\epsilon_0$, conforme indicado por \cite{Goodfellow2016}.

É relevante mencionar que o valor de $\epsilon_0$ pode variar conforme o problema em questão. Em problemas que envolvem um grande número de imagens, como frequentemente ocorre em redes neurais profundas citadas no Capítulo \ref{deep}, valores típicos de $\epsilon_0$ variam entre $1e^{-5}$ e $1e^{-6}$. Esse ajuste é necessário para garantir a eficiência do treinamento e a convergência do modelo.

Por fim, vale mencionar que propostas de decaimento da taxa de aprendizado, como o \textit{Cyclical Learning Rates} (CLR), desenvolvido por \cite{Smith2017CyclicalNetworks}, buscam reduzir o tempo de treinamento e manter a estabilidade dos modelos com a aplicação de variações geométricas. Por exemplo, o modelo triangular busca iniciar o treinamento com baixos valores de taxa de aprendizado e aumentar gradualmente até certo limite, proporcionando uma aceleração de convergência e acompanhado de um processo de diminuição para não perder os mínimos globais em um processo cíclico. Essa proposta sugere uma aceleração da convergência do modelo \citep{Smith2017CyclicalNetworks}, o que pode ser observado através da Figura \ref{deep:optimization:cyclical_lr}.

\begin{figure}[H]
    \centering
    \caption[Comparação de convergência de modelos no CIFAR.]{Comparação de convergência de modelos no \textit{dataset} CIFAR \citep{Krizhevsky2014TheDataset} com uso de taxa de aprendizado cíclica.}
    \includegraphics[width=0.8\linewidth]{recursos/imagens/deep/cifar_lr.png}
    \label{deep:optimization:cyclical_lr}

    Fonte: retirado e adaptado de \cite{Smith2017CyclicalNetworks}.
\end{figure}


\paragraph{Adam}
\label{deep:optimization:adam}

O algoritmo \textit{Adaptive Moment Estimation}, conhecido como Adam, foi proposto por \cite{Kingma2014Adam:Optimization} com o objetivo de otimizar adaptativamente a taxa de aprendizado, integrando conceitos dos algoritmos RMSProp \citep{Hinton2012NeuralDescent} e Momentum \citep{Polyak1964SomeMethods}. A atualização dos parâmetros ocorre por meio do controle da média móvel exponencialmente ponderada de cada algoritmo mencionado, utilizando os parâmetros $\rho_{1}$ e $\rho_{2}$ em cada iteração para calcular a estimativa de primeira ordem $\boldsymbol{s}$, conforme a Equação \ref{deep:eq:adam_1} \citep{Goodfellow2016}:

\begin{equation}
    \label{deep:eq:adam_1}
    \boldsymbol{s} \leftarrow \rho_{1} \boldsymbol{s} + (1 - \rho_{1})\boldsymbol{g},
\end{equation}
tendo $\boldsymbol{g}$ como gradientes da iteração. E a estimativa de segunda ordem $\boldsymbol{r}$ do gradiente, como expresso na Equação \ref{deep:eq:adam_2} \citep{Goodfellow2016}:

\begin{equation}
    \label{deep:eq:adam_2}
    \boldsymbol{r} \leftarrow \rho_{2} \boldsymbol{r} + (1 - \rho_{2})\boldsymbol{g} \odot \boldsymbol{g}.
\end{equation}

Além disso, é importante mencionar que é recomendado aplicar correção de viés às estimativas obtidas, o que pode ser realizado pelas Equações \ref{deep:eq:adam_3} e \ref{deep:eq:adam_4}, conforme proposto por \cite{Goodfellow2016}:

\begin{equation}
    \label{deep:eq:adam_3}
    \boldsymbol{\hat{s}} \leftarrow \frac{\boldsymbol{s}}{1-\rho_1}
\end{equation}
e
\begin{equation}
    \label{deep:eq:adam_4}
    \boldsymbol{\hat{r}} \leftarrow \frac{\boldsymbol{r}}{1-\rho_2}.
\end{equation}

Dessa forma, ao empregar o algoritmo Adam na otimização dos parâmetros, podemos representar a atualização dos parâmetros conforme demonstrado na Equação \ref{deep:eq:adam_5}:

\begin{equation}
    \label{deep:eq:adam_5}
    \boldsymbol{\theta} \leftarrow \boldsymbol{\theta} + \left(-\epsilon \frac{\boldsymbol{\hat{s}}}{\sqrt{\boldsymbol{\hat{r}}}+\delta}\right),
\end{equation}
onde $\delta$ denota uma constante para estabilizar a divisão.


\subsubsection{Retropropagação}
\label{deep:backprop}

\begin{sloppypar}
Por meio do desenvolvimento de \cite{rumelhart1986learning}, o algoritmo de retropropagação (ou em inglês, \textit{Backpropagation}) foi capaz de contribuir diretamente no desenvolvimento dos modelos de redes neurais profundas, de modo que esse algoritmo conseguiu realizar o reajuste de pesos em camadas ocultas dos modelos. Com o auxílio de uma função de custo (Seção
\ref{deep:cust}), o erro é calculado na saída da rede $(y)$ e há a propagação do mesmo, iniciando da camada de saída em direção para a camada de entrada. Isso é realizado através do trabalho conjunto do algoritmo \textit{backpropagation} e de um algoritmo de otimização (Seção \ref{deep:optimization}), em que o \textit{backpropagation} realiza um cálculo rápido de gradientes utilizados pelos algoritmos de otimização.
\end{sloppypar}

Dessa forma, a explicação dessa seção se resume em processos em que, dado o erro $E$ que é obtido a partir da saída da rede $(y)$ em relação a um \textit{ground truth} $(g)$, a primeira etapa é realizada com o cálculo da derivada parcial de $E$ em relação a cada neurônio de saída, sendo formada por $\frac{\partial E}{\partial \boldsymbol{y_i}}$ tendo que $i \in \mathbb{K}$ como a representação do tamanho de cada saída. Depois, com a aplicação da regra da cadeia de $E$ em relação às entradas $x_j$, é possível obter a seguinte expressão \citep{rumelhart1986learning}:

\begin{equation}
    \label{deep:eq:11}
    \frac{\partial E}{\partial x_j} = \frac{\partial E}{\partial y_j} . \frac{\partial y_j}{\partial x_j}.
\end{equation}

A partir da expressão $\frac{\partial E}{\partial x_j}$ é possível visualizar que o erro será afetado na saída ($y$) a medida que a entrada ($x$) for alterada, segundo comentado por \cite{rumelhart1986learning}, que também exemplifica essa função na Equação \ref{deep:eq:12}, ao considerar a entrada como função linear, as conexões do neurônio $i$, assim como os pesos dos neurônios $i$ e $j$:

\begin{equation}
    \label{deep:eq:12}
    \frac{\partial E}{\partial y_j} = \sum_j \frac{\partial E}{\partial x_j . w_{ji}}.
\end{equation}


\subsection{Teste}
\label{deep:test}

A etapa de teste está relacionada com a avaliação do modelo treinado nas condições descritas na Seção \ref{deep:train}. Todavia, para o teste deve-se considerar que o novo conjunto de dados seja desconhecido pela rede neural, ou seja, que não haja intersecção com o conjunto de treinamento, visto que o objetivo do treino está em validar a capacidade de abstração em relação à generalização do modelo.

Entendido isso, habitualmente, é encontrado um conjunto de dados com menos exemplos quando se trata do conjunto de testes \citep{Goodfellow2016}, sendo necessário que ambos os conjuntos de dados, tanto o de teste quanto o de treinamento, possuam uma mesma distribuição dos dados.

O processo de teste pode ser realizado entre as épocas do treinamento, assim, avaliando o desempenho do modelo em um conjunto desconhecido de dados e sendo possível avaliar se a rede está ou não melhorando a capacidade de generalização de acordo com o decorrer das épocas.

Assim, nesta seção são descritos alguns dos elementos utilizados para auxiliar nos teste, como as métricas de avaliação (Seção \ref{deep:metrics}) e o método \textit{cross validation} (Seção \ref{deep:cross}), além de fenômenos que obstruem o desenvolvimento, como \textit{overfitting} e \textit{underfitting} (Seção \ref{deep:overunder}).


\subsubsection{Métricas de Avaliação}
\label{deep:metrics}

Outro ponto adjutório à avaliação dos modelos, tanto para o conjunto de treinamento quanto para o conjunto de teste, é crucial para entender o desempenho e a eficácia do modelo ao longo do treinamento.  Uma métrica fundamental nesse contexto é a acurácia, frequentemente representada como $Acc$, que mede a proporção de exemplos classificados corretamente pelo modelo. Matematicamente, a acurácia pode ser expressa pela Equação \ref{deep:eq:13}:

\begin{equation}
    \label{deep:eq:13}
    Acc = \frac{\text{Quantidade de predições corretas}}{\text{Quantidade de predições realizadas}}.
\end{equation}

A acurácia, como demonstrado na Equação \ref{deep:eq:13}, basicamente se resume entre a razão de acertos pela quantidade total de exemplos no conjunto e é adequada para situações em que se tem classes balanceadas. Mas, quando se tratam de classes desbalanceadas, o modelo em questão pode apresentar fenômenos de \textit{underfitting} ou \textit{overfitting}, como será explicado na Seção \ref{deep:overunder}. Esses fenômenos podem ser identificados ao observar o desempenho da acurácia tanto no conjunto de treinamento quanto no conjunto de teste.

Dessa forma, é desejado obter $Acc_{treino} \approx Acc_{teste}$, e com  acurácias altas. Sendo que as situações de \textit{overfitting} possuem a característica de casos em que $Acc_{treino}$ é exacerbadamente mais alta que $Acc_{teste}$. Por outro lado, tendo $Acc_{treino}$ muito baixa, é comum em casos de \textit{underfitting}.

Em situações de problemas relacionadas à regressão, é comum utilizar a métrica de Erro Quadrático Médio (ou MSE), conforme demonstrado na Equação \ref{deep:eq:9}. Essa métrica é amplamente empregada na avaliação de modelos de regressão devido à sua capacidade de quantificar a diferença entre os valores previstos pelo modelo e os valores reais observados nos dados de teste.

\subsubsection{Validação Cruzada}
\label{deep:cross}

Quando se trata na divisão entre os \textit{datasets} de treino e teste, habitualmente é comentado sobre a configuração de 20\% para teste e 80\% para treino. Todavia, o fato de utilizar um conjunto fixo implica em uma incerteza estatística sobre a média estimada de erro do teste, visto que pode ocorrer um desbalanceamento imprevisto \citep{Goodfellow2016}.

Dessa forma, a validação cruzada, conhecida como \textit{cross validation} em inglês, opera com o princípio de dividir o conjunto de dados em subconjuntos que compõem o conjunto original, permitindo cálculos iterativos por meio de seleções amostrais. Entre as técnicas mais comuns de \textit{cross validation} para essa divisão, destacam-se os métodos \textit{k-fold} e \textit{leave-one-out}. Ambos mantêm a premissa de dividir aleatoriamente o conjunto de dados em subconjuntos de treinamento e teste, possibilitando a avaliação do algoritmo quanto à sua capacidade de generalização. O método \textit{k-fold} divide o conjunto de dados em $k$ grupos (ou dobras), onde cada grupo é utilizado uma vez como conjunto de teste enquanto os outros $k-1$ grupos são usados como conjunto de treinamento. Já o método \textit{leave-one-out} é uma forma extrema de \textit{k-fold} onde cada amostra é usada uma vez como conjunto de teste, enquanto todas as outras amostras formam o conjunto de treinamento. Essas técnicas são fundamentais para garantir uma avaliação robusta dos modelos de aprendizado de máquina, ajudando a estimar sua capacidade de generalização para dados não vistos.

Dessa forma, o erro ($\varepsilon$) é computado no teste ($T$) pela média dos erros individuais ($\varepsilon_i$) em relação ao número de interações ($N$), como demonstrado na Equação \ref{deep:eq:15}:

\begin{equation}
    \label{deep:eq:15}
    \varepsilon_T = \frac{1}{N} \sum_{i=1}^{N} \varepsilon_i.
\end{equation}


\subsubsection{\textit{Overfitting} e \textit{Underfitting}}
\label{deep:overunder}

As situações de \textit{overfitting} e \textit{underfitting} são frequentemente observadas quando o modelo apresenta alta variância. No caso do \textit{overfitting}, o modelo tende a memorizar os dados do conjunto de treinamento, perdendo a capacidade de generalização para novos dados, como os do conjunto de teste. Por outro lado, o \textit{underfitting} ocorre quando o modelo não é capaz de aprender os padrões presentes nos dados de treinamento, resultando em um desempenho abaixo do esperado e uma baixa acurácia, que geralmente é definida pelo desenvolvedor como um critério mínimo de desempenho aceitável. Esses problemas podem prejudicar a eficácia do modelo em lidar com novos dados e comprometer sua utilidade em aplicações do mundo real.

A partir da representação na Figura \ref{deep:fig:5} é possível exemplificar os fenômenos supracitados, em que na Figura \ref{deep:fig:5.1} se visualiza uma situação característica de \textit{underfitting} ao tentar utilizar um modelo linear para um tipo de problema não linear. Já na imagem \ref{deep:fig:5.2} se encontra uma situação em que há uma função que é utilizada adequadamente para a situação, tendo esta convergido e com uma boa adaptação para demais situações com contextos similares. E, por fim, há a representação de uma situação com \textit{overfitting} na Figura \ref{deep:fig:5.3}, em que o conjunto está decorado e o modelo se ajustou demais ao conjunto de dados - situação que acontece devido ao uso de funções complexas para situações simples.

\begin{figure}[H]
   \caption{Fenômenos de \textit{underfitting} e \textit{overfitting}.}
   \centering
   \label{deep:fig:5}
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=1.5in]{recursos/imagens/deep/under.png}
        \caption{Fenômeno de \textit{underfitting}.}
        \label{deep:fig:5.1}
    \end{subfigure}
    ~ 
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=1.5in]{recursos/imagens/deep/apx.png}
        \caption{Representação de função adequada.}
        \label{deep:fig:5.2}
    \end{subfigure}
    ~
    \begin{subfigure}[t]{0.45\textwidth}
        \centering
        \includegraphics[height=1.5in]{recursos/imagens/deep/over.png}
        \caption{Fenômeno de \textit{overfitting}.}
        \label{deep:fig:5.3}
    \end{subfigure}

    Fonte: retirado e adaptado de \cite{Goodfellow2016}.
\end{figure}

Alerta-se que em situações em que a função de custo de treinamento continua decrescente e a função de custo de teste começa a aumentar, é provavel que o modelo esteja iniciando uma circunstância de \textit{overfitting}, já que casos de \textit{underfitting} ocorrem quando ambas as funções de custo permanecem altas mesmo após muitas épocas.

Sendo assim, \textit{underfitting} e \textit{overfitting}, quando presentes, devem ser corrigidos, embora os ajustes dos métodos à complexidade da situação ainda sejam feitos a partir de tentativa e erro e com o auxílio de gráficos \citep{Goodfellow2016}.

\subsection{Considerações Finais do Capítulo}
\label{deep:conclusion}

Neste Capítulo \ref{deep}, foram abordados temas desde a concepção inicial das redes neurais, apresentando as ideias propostas por \cite{mcculloch1943logical}, até detalhes gerais do funcionamento das redes neurais. No entanto, é importante ressaltar que, após o surgimento das \textit{deep learnings}, algumas redes foram direcionadas para resolver problemas específicos. Um exemplo é o uso generalizado das Convolutional Neural Networks (CNNs) no contexto de processamento de imagens e informações espaciais, tópico que será discutido no próximo capítulo.